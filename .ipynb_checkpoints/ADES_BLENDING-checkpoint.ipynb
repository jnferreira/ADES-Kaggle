{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"dev.csv\")\n",
    "test_df = pd.read_csv(\"compete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['is_host_login', 'num_outbound_cmds'], axis=1);\n",
    "test_df = test_df.drop(['is_host_login', 'num_outbound_cmds'], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=['protocol_type'])\n",
    "test_df = pd.get_dummies(test_df, columns=['protocol_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "cat_cols = ['service', 'flag']\n",
    "for col in cat_cols:\n",
    "    if col in train_df.columns:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(train_df[col].astype(str).values) + list(test_df[col].astype(str).values))\n",
    "        train_df[col] = le.transform(list(train_df[col].astype(str).values))\n",
    "        test_df[col] = le.transform(list(test_df[col].astype(str).values))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features dropped: ['num_root', 'srv_serror_rate', 'srv_rerror_rate', 'dst_host_same_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'protocol_type_icmp']\n"
     ]
    }
   ],
   "source": [
    "numerical_features = list(train_df.columns[train_df.dtypes != object].values[:-1])\n",
    "categorical_features = list(train_df.columns[train_df.dtypes == object].values)\n",
    "\n",
    "corr_table = train_df.corr()\n",
    "triu = corr_table.where(np.triu(np.ones(corr_table.shape) ,k=1).astype(np.bool))\n",
    "to_drop = [feat for feat in triu.columns if any(triu[feat] > 0.95)]\n",
    "\n",
    "train_df = train_df.drop(to_drop, axis=1)\n",
    "\n",
    "for feat in to_drop:\n",
    "    if feat in categorical_features:\n",
    "        categorical_features.remove(feat)\n",
    "    else:\n",
    "        numerical_features.remove(feat)\n",
    "\n",
    "print(f'\\nFeatures dropped: {to_drop}')\n",
    "# plt.figure(figsize=(50, 30))\n",
    "# _ = sns.heatmap(corr_table, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df.drop('class', axis=1)\n",
    "y = train_df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_regressor = xgb.XGBRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'iid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-20b81d1dc4f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m bayes_cv_tuner = BayesSearchCV(\n\u001b[0m\u001b[0;32m      5\u001b[0m     estimator = xgb.XGBClassifier(\n\u001b[0;32m      6\u001b[0m         \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, estimator, search_spaces, optimizer_kwargs, n_iter, scoring, fit_params, n_jobs, n_points, iid, refit, cv, verbose, pre_dispatch, random_state, error_score, return_train_score)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         super(BayesSearchCV, self).__init__(\n\u001b[0m\u001b[0;32m    310\u001b[0m              \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m              \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrefit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'iid'"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "        silent=1,\n",
    "        tree_method='approx'\n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'max_depth': (0, 50),\n",
    "        'max_delta_step': (0, 20),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': (0, 5),\n",
    "        'n_estimators': (50, 100),\n",
    "        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = 10,\n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "result = bayes_cv_tuner.fit(X.values, y.values, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-2b45d26ee107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxgb_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "xgb_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-d5abffe8a9c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "params = xgb_bo.res['max']['max_params']\n",
    "params['max_depth'] = int(params['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_opt = xgb.XGBRegressor(learning_rate=0.01,\n",
    "                           n_estimators=6000,\n",
    "                           max_depth=4,\n",
    "                           min_child_weight=0,\n",
    "                           gamma=0.6,\n",
    "                           subsample=0.7,\n",
    "                           colsample_bytree=0.7,\n",
    "                           objective='reg:squarederror',\n",
    "                           nthread=-1,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=27,\n",
    "                           reg_alpha=0.00006,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=6000,\n",
    "                                learning_rate=0.01,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                         num_leaves=6,\n",
    "                         learning_rate=0.01, \n",
    "                         n_estimators=7000,\n",
    "                         max_bin=200, \n",
    "                         bagging_fraction=0.8,\n",
    "                         bagging_freq=4, \n",
    "                         bagging_seed=8,\n",
    "                         feature_fraction=0.2,\n",
    "                         feature_fraction_seed=8,\n",
    "                         min_sum_hessian_in_leaf = 11,\n",
    "                         verbose=-1,\n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "cv_sets = ShuffleSplit(random_state = 4) # shuffling our data for cross-validation\n",
    "parameters = {'n_estimators':range(5, 950, 5), \n",
    "              'min_samples_leaf':range(20, 40, 5), \n",
    "              'max_depth':range(3, 5, 1)}\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "n_iter_search = 10\n",
    "grid_obj = RandomizedSearchCV(rf_regressor, \n",
    "                              parameters, \n",
    "                              n_iter = n_iter_search, \n",
    "                              scoring = scorer, \n",
    "                              cv = cv_sets,\n",
    "                              random_state= 99)\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "rf_opt = grid_fit.best_estimator_\n",
    "\n",
    "print(\"best params: \" + str(grid_fit.best_estimator_))\n",
    "print(\"best params: \" + str(grid_fit.best_params_))\n",
    "print('best score:', grid_fit.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(n_estimators=100, \n",
    "                               random_state=7)\n",
    "rf_opt = RandomForestRegressor(n_estimators=1200,\n",
    "                               max_depth=15,\n",
    "                               min_samples_split=5,\n",
    "                               min_samples_leaf=5,\n",
    "                               max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imp = RandomForestRegressor(n_estimators=1200,\n",
    "                               max_depth=15,\n",
    "                               min_samples_split=5,\n",
    "                               min_samples_leaf=5,\n",
    "                               max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=42)\n",
    "rf_imp.fit(X_train, y_train)\n",
    "importances = rf_imp.feature_importances_\n",
    "df_param_coeff = pd.DataFrame(columns=['Feature', 'Coefficient'])\n",
    "for i in range(len(X_train.columns)-1):\n",
    "    feat = X_train.columns[i]\n",
    "    coeff = importances[i]\n",
    "    df_param_coeff.loc[i] = (feat, coeff)\n",
    "df_param_coeff.sort_values(by='Coefficient', ascending=False, inplace=True)\n",
    "df_param_coeff = df_param_coeff.reset_index(drop=True)\n",
    "print(\"Top 10 features:\\n{}\".format(df_param_coeff.head(10)))\n",
    "\n",
    "importances = rf_imp.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] # Sort feature importances in descending order\n",
    "names = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances\n",
    "plt.figure(figsize=(15, 7)) # Create plot\n",
    "plt.title(\"Top 10 Most Important Features\") # Create plot title\n",
    "plt.bar(range(10), importances[indices][:10]) # Add bars\n",
    "plt.xticks(range(10), names[:10], rotation=90) # Add feature names as x-axis labels\n",
    "#plt.bar(range(X_train.shape[1]), importances[indices]) # Add bars\n",
    "#plt.xticks(range(X_train.shape[1]), names, rotation=90) # Add feature names as x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=7)\n",
    "rcv_alphas = np.arange(14, 16, 0.1)\n",
    "ridge = RidgeCV(alphas=rcv_alphas, \n",
    "                cv=kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(C= 20, \n",
    "          epsilon= 0.008, \n",
    "          gamma=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen = StackingCVRegressor(regressors=(gbr,\n",
    "                                            xgb_opt,\n",
    "                                            lightgbm,\n",
    "                                            rf_opt,\n",
    "                                            ridge, \n",
    "                                            svr),\n",
    "                                meta_regressor=xgb_opt,\n",
    "                                use_features_in_secondary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting models to the training data:')\n",
    "\n",
    "print('xgboost....')\n",
    "xgb_model_full_data = xgb_opt.fit(X, y)\n",
    "print('GradientBoosting....')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "print('lightgbm....')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)\n",
    "print('RandomForest....')\n",
    "rf_model_full_data = rf_opt.fit(X, y)\n",
    "print('Ridge....')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "print('SVR....')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "print('Stacking Regression....')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_models_predict(X):\n",
    "    return ((0.25 * stack_gen_model.predict(np.array(X))) + \\\n",
    "            (0.25 * gbr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * svr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.1 * ridge_model_full_data.predict(X))+ \\\n",
    "            (0.05 * xgb_model_full_data.predict(X)) + \\\n",
    "            (0.05 * rf_model_full_data.predict(X)) \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(to_drop, axis=1)\n",
    "test_id = test_df.Id.values\n",
    "test_df = test_df.drop(\"Id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the blend\n",
    "y_pred_final = np.floor(np.expm1(blend_models_predict(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'Id': test_id, 'class':preds})\n",
    "submit.to_csv('kmeans_lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
